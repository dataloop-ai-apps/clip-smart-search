{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa15f0ac-d402-45c4-87e5-afa19b7d6bee",
   "metadata": {},
   "source": [
    "# Using CLIP to query an image dataset via text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02dc471-eb87-4c2a-813b-80c1c51331c0",
   "metadata": {},
   "source": [
    "This notebook describes how to text query an image dataset using pretrained and fine-tuned OpenAI's multi-modal CLIP model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff65cba-85dc-48ea-be5c-d3ba02e94046",
   "metadata": {},
   "source": [
    "## Notebook setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "295ae669-fb10-4455-ba9a-e5a14009718a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yaya Tang\\PycharmProjects\\clip-smart-search\\venv\\lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Yaya Tang\\PycharmProjects\\clip-smart-search\\venv\\lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Yaya Tang\\PycharmProjects\\clip-smart-search\\venv\\lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Yaya Tang\\PycharmProjects\\clip-smart-search\\venv\\lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import dtlpy as dl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "from bokeh.palettes import Viridis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41f2810-6f41-4fc7-80b8-157178b5c040",
   "metadata": {},
   "source": [
    "## Load model, images\n",
    "\n",
    "Images are directly loaded from an file directory, and then compiled as Image objects for later embedding or training.\n",
    "Check that every filepath is an image, and open and add to the images list for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3e7044-1b34-46c5-a530-bc631d69bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = r\"output\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# load saved, trained model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d38a34-310e-4b83-b79d-cf94125a0c5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\Yaya Tang\\Documents\\DATASETS\\TACO dataloop\\items\\raw\\000047.jpg...[2023-08-09 15:36:14][ERR][dtlpy:v1.81.4][services.api_client:935] POST https://gate.dataloop.ai/api/v1/sdk/check\n",
      "User-Agent: dtlpy/1.81.4 CPython/3.8.10 Windows/10\n",
      "Content-Length: 41\n",
      "Content-Type: application/x-www-form-urlencoded\n",
      "version=1.81.4&email=yaya.t%40dataloop.ai\n",
      "Processing C:\\Users\\Yaya Tang\\Documents\\DATASETS\\TACO dataloop\\items\\raw\\batch_7__0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(img_paths))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m img_paths:\n\u001b[1;32m---> 13\u001b[0m     img_np \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     images_np\u001b[38;5;241m.\u001b[39mappend(preprocess(img_np))\n\u001b[0;32m     15\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\PycharmProjects\\clip-smart-search\\venv\\lib\\site-packages\\PIL\\Image.py:911\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    865\u001b[0m ):\n\u001b[0;32m    866\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 911\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    913\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    915\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\clip-smart-search\\venv\\lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def is_image_file(filename):\n",
    "    img_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff']\n",
    "    return any(filename.lower().endswith(ext) for ext in img_extensions)\n",
    "\n",
    "# load images to embed and query\n",
    "img_dir = Path(r\"C:\\Users\\Yaya Tang\\Documents\\DATASETS\\TACO dataloop\\items\\raw\")\n",
    "img_paths = [str(img_path) for img_path in img_dir.glob(\"*\") if is_image_file(str(img_path))]\n",
    "\n",
    "# prepare images\n",
    "images_np = []\n",
    "pbar = tqdm.tqdm(total=len(img_paths))\n",
    "for img_path in img_paths:\n",
    "    img_np = Image.open(img_path).convert(\"RGB\")\n",
    "    images_np.append(preprocess(img_np))\n",
    "    pbar.set_description(f\"Processing {img_path}...\")\n",
    "    pbar.update()\n",
    "print(len(img_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5604015-0c22-4162-b469-1042e5d685c5",
   "metadata": {},
   "source": [
    "## Generate descriptions for images from labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa1ba34-ca91-4cba-9a34-19c964a2f88b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "A description is generated for the dataset based on the objects detected in the image, and will be printed to help assess model efficacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a9928-2998-4745-add6-ddfae08c6fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create descriptions for each image from its annotations\n",
    "import dtlpy as dl\n",
    "\n",
    "def create_descrip(labels: list):\n",
    "    description = \"a photo\"\n",
    "    if len(labels) != 0:\n",
    "        description += \" of a \"\n",
    "        for i, label in enumerate(labels):\n",
    "            if i < len(labels) - 1:\n",
    "                description += f\"{label}, \"\n",
    "            else:\n",
    "                description += f\"and {label}.\"\n",
    "    return description\n",
    "\n",
    "# setup dtlpy\n",
    "dl.setenv('prod')\n",
    "if dl.token_expired():\n",
    "    dl.login()\n",
    "\n",
    "dl_dataset = dl.datasets.get(dataset_id='64c27e74615b1c5d7d576776')\n",
    "\n",
    "# for training, adding annotations as descriptions\n",
    "all_labels = dl_dataset.labels\n",
    "new_label_names = [label.tag for label in all_labels]\n",
    "\n",
    "# create text descriptions from labels\n",
    "items = list(dl_dataset.items.list().all())\n",
    "item_labels_lookup = {}\n",
    "pbar = tqdm.tqdm(total=len(items))\n",
    "for item in items:\n",
    "    item_name = item.name\n",
    "    annotations = item.annotations.list()\n",
    "    item_labels = []\n",
    "    for annotation in annotations:\n",
    "        item_labels.append(str(annotation.label).split(\".\")[-1])\n",
    "    item_labels_lookup[item_name] = item_labels\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e55f3-5509-46db-974d-fb8a72f91c5f",
   "metadata": {},
   "source": [
    "## Create image embeddings\n",
    "\n",
    "Use the loaded CLIP model to create a feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b3668c-9b05-4219-8839-616d45fe4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = torch.tensor(np.stack(images_np)).to(device)\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e5f13-56f1-499e-8678-101ac740c2d6",
   "metadata": {},
   "source": [
    "## Query images with pretrained CLIP\n",
    "\n",
    "Enter the query to create its embedding so we can find the nearest images in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb46193-8132-4e43-ac5a-47ef3af7784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create query feature\n",
    "QUERY_STRING = \"cigarettes on the sidewalk\"\n",
    "NUM_RESULTS = 20\n",
    "\n",
    "text_tokens = clip.tokenize([QUERY_STRING]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa0ee5-cc54-4c52-8999-bc9d17569af7",
   "metadata": {},
   "source": [
    "### Get the K nearest images to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98000e1-a70a-4461-9bdd-b0e27796b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top results\n",
    "results = cosine_similarity(text_features.cpu().numpy(), image_features.cpu().numpy())\n",
    "\n",
    "results_dict = {'name': [], 'prob': [], 'filepath': []}\n",
    "\n",
    "pbar = tqdm.tqdm(total=len(img_paths))\n",
    "for i, img_path in enumerate(img_paths):\n",
    "    results_dict['name'].append(Path(img_path).name)\n",
    "    results_dict['prob'].append(results[0][i])\n",
    "    results_dict['filepath'].append(img_path)\n",
    "    results_dict.update()\n",
    "    pbar.update()\n",
    "\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "results_df.sort_values(by=['prob'], ascending=False, inplace=True)\n",
    "\n",
    "results_df = results_df.iloc[:NUM_RESULTS][['name', 'prob', 'filepath']]\n",
    "print(results_df[['name', 'prob']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb194df9-d56a-4161-b4c9-d3ec50ee2558",
   "metadata": {},
   "source": [
    "### Plot the returned images from query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da7fec-dd69-4c2b-9698-ca8cedccb64d",
   "metadata": {},
   "source": [
    "Show the top N most similar images for this text/query (user defined above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87889123-e192-4bb9-b71b-2f29ddd4b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model text features\n",
    "model = model.eval()\n",
    "\n",
    "text_tokens = clip.tokenize([QUERY_STRING]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a919c0-276a-4217-bd7b-17c9c792323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top results\n",
    "result = cosine_similarity(text_features.cpu().numpy(), image_features.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44a40b-8478-4c2c-97c3-18e3b2d03f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to df\n",
    "results_dict = {'name': [], 'prob': [], 'filepath': []}\n",
    "\n",
    "pbar = tqdm.tqdm(total=len(img_paths))\n",
    "for i, img_path in enumerate(img_paths):\n",
    "    results_dict['name'].append(Path(img_path).name)\n",
    "    results_dict['prob'].append(result[0][i])\n",
    "    results_dict['filepath'].append(img_path)\n",
    "    results_dict.update()\n",
    "    pbar.update()\n",
    "\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "results_df.sort_values(by=['prob'], ascending=False, inplace=True)\n",
    "\n",
    "results_df = results_df.iloc[:NUM_RESULTS][['name', 'prob', 'filepath']]\n",
    "print(results_df[['name', 'prob']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa194acb-bbcb-47e1-9db4-7b0ef5d6a85c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot returned images on a grid\n",
    "num_grid = math.isqrt(NUM_RESULTS)\n",
    "subplot_dims = num_grid + 1 if num_grid ** 2 < NUM_RESULTS else num_grid\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.tight_layout()\n",
    "for i, img_path in enumerate(results_df['filepath'].iloc[:NUM_RESULTS]):\n",
    "    plt.subplot(subplot_dims, subplot_dims, i + 1)\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    plt.text(0,-1, f'{Path(img_path).name}', verticalalignment=\"bottom\")\n",
    "    plt.text(0,0, f'{item_labels_lookup[Path(img_path).name]}', verticalalignment=\"top\")\n",
    "    plt.imshow(image)\n",
    "\n",
    "plt.suptitle(f\"Query: {QUERY_STRING}, found {len(results_df)}, on pretrained CLIP\")\n",
    "\n",
    "plot_filename = f\"clip_query_results_PRETRAINED_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.png\"\n",
    "save_path = os.path.join(save_dir, plot_filename)\n",
    "plt.savefig(save_path, dpi=800)\n",
    "print(f'Saved query results to {save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0afe8d-1e7e-404d-977a-fc103d0d2038",
   "metadata": {},
   "source": [
    "### Visualize: Dimensionality reduction with UMAP \n",
    "\n",
    "For UMAP dimension reduction, we can take the image features (n_images, 512) to (n_images, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1aeb79-4137-4950-9e02-8efae2d8dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate both image + query features and reduce with UMAP\n",
    "\n",
    "all_features = torch.cat((image_features, text_features), 0)\n",
    "reducer = umap.UMAP(random_state=42, metric='cosine')\n",
    "embedding = reducer.fit_transform(all_features.cpu())\n",
    "\n",
    "# output_file(filename=r\"dataloop\\output\\interactive_umap_TACO.html\",\n",
    "#             title=\"TACO umap plotting fastdup features\")\n",
    "\n",
    "# update lists to include the query string as the last item\n",
    "names = [Path(path).name for path in img_paths]\n",
    "results = results_df['name'].tolist()\n",
    "query_returned = ['results' if name in results else '0' for name in names]\n",
    "\n",
    "names.append('query')\n",
    "query_returned.append('query')\n",
    "img_paths.append('query')\n",
    "\n",
    "thumbs_df = pd.DataFrame(embedding, columns=['x', 'y'])\n",
    "thumbs_df['filename'] = img_paths\n",
    "thumbs_df['name'] = names\n",
    "thumbs_df['query_returned'] = query_returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4c3a3e-d500-476a-955e-07e6c9aa6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of img paths that are relevant for the results\n",
    "names = [Path(path).name for path in img_paths]\n",
    "results = results_df['name'].tolist()\n",
    "query_returned = ['results' if name in results else '0' for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e220d-8c29-48a2-bbf8-30447e12d131",
   "metadata": {},
   "source": [
    "Now plot the feature vectors in 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bee14-8d82-480b-b3ea-90eca415f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(x=thumbs_df['x'], y=thumbs_df['y'], hue=np.array(thumbs_df['query_returned']), palette=\"deep\", markers=[\"o\", \"*\"])\n",
    "plt.axis('off')\n",
    "plt.title('UMAP of pretrained CLIP features, with returned images')\n",
    "plt.show()\n",
    "\n",
    "plot_filename = f\"umap_TACO_returned_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.png\"\n",
    "save_path = os.path.join(save_dir, plot_filename)\n",
    "plt.savefig(save_path, dpi=800)\n",
    "print(f'Saved UMAP fig to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32200248-3e60-4a84-a0f9-6bce00270ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot only returned images\n",
    "df_filt = thumbs_df[thumbs_df['query_returned'] != '0']\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(x=df_filt['x'], y=df_filt['y'], hue=np.array(df_filt['query_returned']), palette=\"deep\")\n",
    "plt.axis('off')\n",
    "plt.title(f'UMAP, CLIP pretrained features, only returned images for query {QUERY_STRING}')\n",
    "\n",
    "plot_filename = f\"umap_TACO_returned_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.png\"\n",
    "save_path = os.path.join(save_dir, plot_filename)\n",
    "plt.savefig(save_path, dpi=800)\n",
    "print(f'Saved UMAP fig to {save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d4580c-388c-4a31-9a82-1c1e2831d3a6",
   "metadata": {},
   "source": [
    "### Random script for changing plot point markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c0617-e0a7-4b1f-942c-43beaadfca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x = np.random.rand(100)\n",
    "# y = np.random.rand(100)\n",
    "# category = np.random.randint(0, 3, 100)\n",
    "\n",
    "# markers = ['s', 'o', 'h', '+']\n",
    "# for k, m in enumerate(markers):\n",
    "#     i = (category == k)\n",
    "#     print(m)\n",
    "#     plt.scatter(x[i], y[i], marker=m)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f581e5-7643-44f7-8b2f-85ab6b151826",
   "metadata": {},
   "source": [
    "# Interactive visualization\n",
    "\n",
    "Now plot it using an interactive visualization that shows the images in feature vector space. For example, now you can look at the images closest to the query embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae33d75-3b4f-4059-af51-959d608ab507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################\n",
    "# # interactive viz #\n",
    "# ###################\n",
    "# # copied from kaggle nb\n",
    "# dataset.transform = None\n",
    "# image_urls = b64_image_files(images_generator(dataset))\n",
    "\n",
    "# source = ColumnDataSource(data=dict(\n",
    "#     x=thumbs_df['x'],\n",
    "#     y=thumbs_df['y'],\n",
    "#     # label=[str(l) for l in labels],\n",
    "#     # prediction=[str(p) for p in predictions],\n",
    "#     # success=[str(s) for s in success],\n",
    "#     # desc=descs,\n",
    "#     imgs=img_paths,\n",
    "#     # image_urls=image_urls,\n",
    "# ))\n",
    "\n",
    "# TOOLTIPS = \"\"\"\n",
    "#     <div>\n",
    "#         <div>\n",
    "#             <img\n",
    "#                 src=\"@image_urls\" height=\"200\" alt=\"@image_urls\" width=\"200\"\n",
    "#                 style=\"float: left; margin: 0px 15px 15px 0px;\"\n",
    "#                 border=\"2\"\n",
    "#             ></img>\n",
    "#         </div>\n",
    "#         <div>\n",
    "#             <div style=\"font-size: 15px; font-weight: bold;\">Label: @label</div>\n",
    "#             <div style=\"font-size: 15px; font-weight: bold;\">Predicted: @prediction</div>\n",
    "#             <div style=\"font-size: 12px; font-weight: bold;\">Success: @success</div>\n",
    "#             <div style=\"font-size: 12px;\">@desc</div>\n",
    "#             <div style=\"font-size: 12px; color: #966;\">[$index]</div>\n",
    "#         </div>\n",
    "#     </div5\n",
    "# \"\"\"\n",
    "\n",
    "# p = figure(plot_width=1000, plot_height=600, tooltips=TOOLTIPS,\n",
    "#            title=\"UMAP: Mouse over the dots\")\n",
    "\n",
    "# mapper = factor_cmap(field_name='label', palette=Category10[5], factors=['0', '1', '2', '3', '4'])\n",
    "\n",
    "# p.scatter('x', 'y',\n",
    "#           color=mapper,\n",
    "#           marker=factor_mark('success', ['circle', 'x'], [str(True), str(False)]),\n",
    "#           size=10,\n",
    "#           fill_alpha=0.5,\n",
    "#           legend_field=\"desc\",\n",
    "#           source=source)\n",
    "\n",
    "# p.legend.orientation = \"vertical\"\n",
    "# p.legend.location = \"top_right\"\n",
    "# output_file(\"umap.html\")\n",
    "# show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94f0c0-cd55-46cf-8079-c31d25424a27",
   "metadata": {},
   "source": [
    "# Fine-tune CLIP on TACO 100\n",
    "\n",
    "To improve the results, we fine-tune CLIP on 100 images with descriptions and see if the results are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda2905-4c06-4e63-91ab-5257197b9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "MODEL_VERSION = \"1_TACO100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97e1ad-7f6e-49b5-8540-5c8c91869d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    data_pairs = pd.read_csv(r\"C:\\Users\\Yaya Tang\\Documents\\DATASETS\\TACO 100\\taco_100_INPUTS_nb.csv\")\n",
    "    return data_pairs['filepath'], data_pairs['img_description']\n",
    "\n",
    "class image_title_dataset(Dataset):\n",
    "    def __init__(self, list_image_path, list_txt):\n",
    "        self.image_path = list_image_path\n",
    "        # you can tokenize everything at once in here(slow at the beginning), or tokenize it in the training loop.\n",
    "        self.title = clip.tokenize(list_txt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = preprocess(Image.open(self.image_path[idx]))  # Image from PIL module\n",
    "        title = self.title[idx]\n",
    "        return image, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f0a1f-8560-4e47-8a9d-d342ddb30799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "random_seed = 11\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "list_image_path, list_txt = get_data()\n",
    "dataset = image_title_dataset(list_image_path, list_txt)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "def convert_models_to_fp32(model):\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.float()\n",
    "        p.grad.data = p.grad.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c0f7c-a410-4093-b08e-81ba8a10f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "# if model iteration name already exists, skip and load instead\n",
    "try:\n",
    "    checkpoint_path = rf\"C:\\Users\\Yaya Tang\\PycharmProjects\\clip-smart-search\\checkpoints\\model_{MODEL_VERSION}_BEST.pt\"\n",
    "    \n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)  # Must set jit=False for training\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model {MODEL_VERSION} loaded\")\n",
    "except FileNotFoundError:\n",
    "    if device == \"cpu\":\n",
    "        model.float()\n",
    "    else:\n",
    "        clip.model.convert_weights(model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "    \n",
    "    # keep track of the best model\n",
    "    EARLY_STOPPING = 10 \n",
    "    best_loss = np.Inf\n",
    "    best_iter = 0\n",
    "    end_training = False\n",
    "    \n",
    "    # loss fxns for images and their descriptions\n",
    "    loss_img = nn.CrossEntropyLoss()\n",
    "    loss_txt = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, betas=(0.9, 0.98), eps=1e-6,\n",
    "                                 weight_decay=0.2)  # Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        pbar = tqdm.tqdm(dataloader, total=len(dataloader))\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            images, texts = batch\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "    \n",
    "            # forward pass\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "    \n",
    "            # calc loss + backprop\n",
    "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "            total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "            total_loss.backward()\n",
    "            if device == \"cpu\":\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                convert_models_to_fp32(model)\n",
    "                optimizer.step()\n",
    "                clip.model.convert_weights(model)\n",
    "    \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {total_loss.item():.4f}\")\n",
    "        pbar.update()\n",
    "        \n",
    "        \n",
    "        if epoch == 0:\n",
    "            best_loss = total_loss # val_loss\n",
    "        elif total_loss < best_loss: # val_loss < best_loss:\n",
    "            best_iter = epoch + 1\n",
    "            best_loss = total_loss # val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': total_loss,  # val_loss,\n",
    "            }, rf\"C:\\Users\\Yaya Tang\\PycharmProjects\\clip-smart-search\\checkpoints\\model_{MODEL_VERSION}_BEST.pt\")\n",
    "    \n",
    "        if ((epoch + 1) - best_iter) > EARLY_STOPPING:\n",
    "            print(\"Early stop achieved at\", epoch + 1)\n",
    "            break\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': total_loss,\n",
    "        }, rf\"C:\\Users\\Yaya Tang\\PycharmProjects\\clip-smart-search\\checkpoints\\model_{MODEL_VERSION}_epoch_{epoch + 1}.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906554fa-4c72-43ac-9c53-8635cad305d1",
   "metadata": {},
   "source": [
    "### Re-create image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec4648-cc1c-40e4-b756-11150a04e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image features\n",
    "model = model.eval()\n",
    "image_input = torch.tensor(np.stack(images_np)).to(device)\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d50ef4-7468-416c-89b3-26fc634a6146",
   "metadata": {},
   "source": [
    "### Query images with fine-tuned CLIP\n",
    "\n",
    "We can now take the images that were returned and color them in on the UMAP plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23329781-a3a1-443f-b124-351521d76426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create query feature\n",
    "QUERY_STRING = \"cigarettes on the sidewalk\"\n",
    "NUM_RESULTS = 20\n",
    "\n",
    "text_tokens = clip.tokenize([QUERY_STRING]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dfc50c-a1ee-464a-b814-3f1ec2553339",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bd48f7-5f74-4cc9-8278-aea70061a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top results\n",
    "results = cosine_similarity(text_features.cpu().numpy(), image_features.cpu().numpy())\n",
    "\n",
    "results_dict = {'name': [], 'prob': [], 'filepath': []}\n",
    "\n",
    "pbar = tqdm.tqdm(total=len(img_paths))\n",
    "for i, img_path in enumerate(img_paths):\n",
    "    results_dict['name'].append(Path(img_path).name)\n",
    "    results_dict['prob'].append(results[0][i])\n",
    "    results_dict['filepath'].append(img_path)\n",
    "    results_dict.update()\n",
    "    pbar.update()\n",
    "\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "results_df.sort_values(by=['prob'], ascending=False, inplace=True)\n",
    "\n",
    "results_df = results_df.iloc[:NUM_RESULTS][['name', 'prob', 'filepath']]\n",
    "print(results_df[['name', 'prob']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5561711-a96e-45e8-b435-358210cd4610",
   "metadata": {},
   "source": [
    "## Plot returned images from query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec6ced-2dbd-4b37-b275-0874a2f68419",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_grid = math.isqrt(NUM_RESULTS)\n",
    "subplot_dims = num_grid + 1 if num_grid ** 2 < NUM_RESULTS else num_grid\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.tight_layout()\n",
    "for i, img_path in enumerate(results_df['filepath'].iloc[:NUM_RESULTS]):\n",
    "    plt.subplot(subplot_dims, subplot_dims, i + 1)\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    plt.text(0,-1, f'{Path(img_path).name}', verticalalignment=\"bottom\")\n",
    "    plt.imshow(image)\n",
    "\n",
    "plt.suptitle(f\"Query: {QUERY_STRING}, found {len(results_df)}, on fine-tuned CLIP\")\n",
    "plot_filename = f\"clip_query_results_{MODEL_VERSION}_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.png\"\n",
    "save_path = os.path.join(save_dir, plot_filename)\n",
    "plt.savefig(save_path, dpi=800)\n",
    "print(f'Saved query results to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb2867-325d-4500-ab11-ef5b19b879ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# UMAP reduction and visualization #\n",
    "####################################\n",
    "reducer = umap.UMAP(random_state=42, metric=\"cosine\")\n",
    "embedding = reducer.fit_transform(image_features.cpu())\n",
    "\n",
    "dataset_name = \"TACO\"\n",
    "output_file(filename=rf\"dataloop\\output\\interactive_umap_{dataset_name}.html\",\n",
    "            title=f\"{dataset_name} umap plotting fastdup features\")\n",
    "\n",
    "thumbs_df = pd.DataFrame(embedding, columns=['x', 'y'])\n",
    "thumbs_df['filename'] = img_paths\n",
    "thumbs_df['name'] = thumbs_df['filename'].apply(lambda path: Path(path).name)\n",
    "thumbs_df['query_returned'] = thumbs_df['name'].isin(results_df.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130bc73-792d-4d5f-882e-41d105c5e99b",
   "metadata": {},
   "source": [
    "### Visualize fine-tuned features\n",
    "\n",
    "For UMAP dimension reduction, we can take the image features (n_images, 512) to (n_images, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47cf5a-b2fa-4602-b4f2-8fa431609993",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(x=thumbs_df['x'], y=thumbs_df['y'], hue=np.array(thumbs_df['query_returned']), palette=\"deep\")\n",
    "plt.axis('off')\n",
    "plt.title('UMAP of pretrained CLIP features, with returned images')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(f\"umap_TACO_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70678a84-3cc3-469c-9886-2fb44d7c8c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
